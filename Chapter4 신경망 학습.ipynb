{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOgra1Y2tEQH0NxCP////Wh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chapter4 신경망 학습\n","\n","- 학습 : 훈련 데이터로부터 가중치의 최적값을 자동으로 획득하는 것\n","- 손실 함수 : 신경망이 학습할 수 있도록 해주는 **지표**\n","- 손실 함수의 결과값을 가장 작게 만드는 가중치를 찾는 것이 학습의 목표"],"metadata":{"id":"b4JhU0rqkbsX"}},{"cell_type":"markdown","source":["## 4.1 데이터에서 학습한다\n","\n","- 데이터에서 학습한다 = 가중치의 값을 데이터를 보고 자동으로 결정한다\n","- 이번 장에서는 신경망 학습과 MNIST 데이터셋의 손글씨 숫자를 학습하는 코드를 구현한다\n","\n","- 선형분리가 가능한 문제라면, 데이터로부터 자동으로 학습 가능함 - 유한 번의 학습을 통해 풀 수 있다는 사실 증명 (퍼셉트론 수렴 정리)\n","- 비선형 분리 문제는 자동으로 학습할 수 없음"],"metadata":{"id":"UvCKA8ctk0gn"}},{"cell_type":"markdown","source":["### 4.1.1 데이터 주도 학습\n","\n","- 기계학습은 데이터가 생명이자 중심. 사람의 경험과 직관을 단서로 시행착오를 반복하는 것과 달리 기계학습은 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도한다.\n","\n","- 사람이 인식하기는 쉽지만, 그 패턴(규칙성)을 찾아내기는 쉽지 않음\n","\n","- 데이터 활용 방법의 하나 : 이미지에서 특징 추출하고 그 특징의 패턴을 ML기술로 학습\n","\n","- 이미지의 특징은 보통 벡터로 기술, CV분야에서는 SIFT, SURF, HOG 등의 특징 사용\n","\n","- 이런 특징을 사용하여 이미지를 벡터로 변환, 지도학습의 대표 분류 기법인 SVM, KNN등으로 학습 가능\n","\n","- 규칙 찾아내는 역할은 기계가 담당하나, 문제에 적합한 특징 찾아내는 것은 사람이 해야한다.\n","\n","- 문제에 적합한 특징 사용하지 않으면(설계하지 않으면) 좋은 결과를 얻을 수 없음\n","\n","- 딥러닝은 이미지를 '있는 그대로' 학습 (중요한 특징까지 스스로 학습)\n","\n","- 딥러닝을 '종단간 기계학습(end-to-end machine learning)이라고도 함. \n","\n","  => 데이터(입력)에서 목표한 결과(출력)를 개입없이 얻는다는 뜻"],"metadata":{"id":"wQX8VLz9li3w"}},{"cell_type":"markdown","source":["### 4.1.2 훈련 데이터와 시험 데이터\n","\n","![이미지](https://wikidocs.net/images/page/31947/%EB%8D%B0%EC%9D%B4%ED%84%B0.PNG)\n","\n","- 훈련 데이터를 학습에 사용, 추론에 테스트 데이터 사용\n","\n","- 훈련-시험 데이터를 나누는 이유 : 범용능력을 평가하기 위해\n","\n","  범용능력 : 아직 보지 못한 데이터로도 문제를 올바르게 풀어내는 능력. 데이터셋 하나로만 매개변수의 학습 및 평가를 수행하면 올바른 평가 될 수 없음\n","\n","- 오버피팅(overfitting) : 한 데이터셋에만 지나치게 과적화된 상태"],"metadata":{"id":"DfmMwdBAno-0"}},{"cell_type":"markdown","source":["## 4.2 손실 함수\n","\n","손실함수 : 신경망이 최적의 매개변수 값을 탐색하는 기준 (지표)\n","\n","임의의 함수를 사용할 수 있지만 일반적으로 오차제곱합과 교차 엔트로피 오차 사용\n","손실함수는 신경망 성능의 '나쁨'을 나타내는 지표"],"metadata":{"id":"S7byGcb2oQgI"}},{"cell_type":"markdown","source":["### 4.2.1 오차제곱합\n","\n","![이미지](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FdCDpqv%2Fbtq102Ja1sN%2FadDK0g6c4q8nutIvh9bnyk%2Fimg.jpg)\n"],"metadata":{"id":"EdU6r-NoouHa"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"wihpBx4PXj3B","executionInfo":{"status":"ok","timestamp":1667375838691,"user_tz":-540,"elapsed":36,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"outputs":[],"source":["y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 라벨 0,1,2 순의 확률값들. 2에 해당하는 원소값이 1이므로 정답은 2\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 이렇게 한 원소만 1로 하고 그 외에는 0으로 나타내는 표기를 원-핫 인코딩이라 함"]},{"cell_type":"code","source":["import numpy as np\n","def sum_squares_error(y, t):\n","    return 0.5 * np.sum((y-t)**2)"],"metadata":{"id":"KIWMRL04pYtP","executionInfo":{"status":"ok","timestamp":1667375838694,"user_tz":-540,"elapsed":39,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["sum_squares_error(np.array(y), np.array(t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nr5A1vGyppCW","executionInfo":{"status":"ok","timestamp":1667375838696,"user_tz":-540,"elapsed":40,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"86918ad9-88b0-4449-a867-be2e2a74ee82"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.09750000000000003"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n","sum_squares_error(np.array(y), np.array(t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XrHDpc3opx2Q","executionInfo":{"status":"ok","timestamp":1667375838699,"user_tz":-540,"elapsed":31,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"44bd8d0c-fe3d-4bc7-e595-c0afc05cc5eb"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5975"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["=> 손실 함수 쪽 출력이 작으면 정답 레이블과의 오차도 작다 => 오차제곱합이 더 작은 것이 정답에 더 가깝다"],"metadata":{"id":"aWtot2qEqIpo"}},{"cell_type":"markdown","source":["### 4.2.2 교차 엔트로피 오차\n","\n","![이미지](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FNAFf7%2Fbtq1WKpHPxE%2FEuafGjZOGD0ddh7yHoWybK%2Fimg.jpg)\n","\n","- log는 밑이 e인 자연로그\n","- Yk는 신경망의 출력\n","- tk는 정답 레이블 ( tk는 정답에 해당하는 인덱스만 1인 원-핫 인코딩)\n","\n","\n","실질적으로 정답일 때의 추정의 자연로그 계산 (정답이 아닌 것은 tk가 0 이므로 결과영향X)\n","\n","교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 됨\n","\n","자연로그 y=logx의 그래프 : 정답출력이 커질수록 0에 다가가다가, 그 출력이 1일 때 0이 됨\n","![iamge](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FchMSBp%2Fbtq11yODoRj%2Fsn4Z8DyXUcZpbkELx4lQkk%2Fimg.png)"],"metadata":{"id":"ZQsKsNMeqTAe"}},{"cell_type":"code","source":["def cross_entropy_error(y, t):\n","    delta = 1e-7\n","    return -np.sum(t * np.log(y+delta)) "],"metadata":{"id":"sUFo3mSeqCr5","executionInfo":{"status":"ok","timestamp":1667375838700,"user_tz":-540,"elapsed":28,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["- 왜 -np.sum??? = 확률값이면 0.1, 0.6등등 소수인데 이게 로그안에 들어가면 마이너스가 밖으로 나옴. 그래서 앞에 - 붙여준겨\n","- t를 곱해주는 이유는?? => t는 원-핫 인코딩이 되어있는 상태. np.log(y+delta)를 거치고 나온 넘파이 배열과 t를 곱하면,  t는 정답값을 제외하고 나머지는 다 0이기 때문에 정답에 해당하는 로그값만 남는다 \n","\n","- np.log 계산할 때 아주 작은 값 delta를 더해준 이유는 np.log()에 0을 입력하면 -int가 되어 계산이 안되므로\n","- 0이 되지 않도록 아주 작은 값을 더해주어, 마이너스 무한대가 발생하지 않도록 한 것이다."],"metadata":{"id":"_xYkIwAfg3AX"}},{"cell_type":"code","source":["y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n","t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n","cross_entropy_error(y, t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ACNYeJDorOTI","executionInfo":{"status":"ok","timestamp":1667375838700,"user_tz":-540,"elapsed":27,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"70926c10-9e09-4c11-f817-332dee23eaef"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.510825457099338"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n","cross_entropy_error(y, t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSdvE4gnsG7c","executionInfo":{"status":"ok","timestamp":1667375838701,"user_tz":-540,"elapsed":23,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"de829b75-7713-4024-81c6-bc526644bf10"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.302584092994546"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["=> 교차 엔트로피 오차도 마찬가지로 출력이 작을수록 정답에 가깝다"],"metadata":{"id":"uN1cQ7-asZNC"}},{"cell_type":"markdown","source":["### 4.2.3 미니배치 학습\n","\n","- 기계학습 문제는 훈련 데이터를 사용해 학습한다 \n","\n","  => 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다\n","\n","  => 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 한다\n","\n","  ex) 훈련 데이터가 100개면 100개의 손실 함수 값들의 합을 지표로 삼는다\n","\n","<br/>\n","\n","- ![이미지](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FckGuuv%2Fbtq1XlQuy6b%2F7VMOsKXz7Igni2KtvHVFf1%2Fimg.jpg)\n","\n","- 복잡해 보이지만 앞에서의 손실함수 값들을 다 더하고 전체 데이터 개수로 나누어 '평균 손실 함수'를 구한 것이다\n","\n","- 마지막에 N(데이터의 갯수)으로 나누어 '평균 손실 함수' 구함 ==> 훈련 데이터 개수와 관계없이 언제든 통일된 지표 얻을 수 있다"],"metadata":{"id":"WwdX4UuhwqlR"}},{"cell_type":"markdown","source":["- MNIST의 경우 - 훈련 데이터 60000개. 모든 데이터를 대상으로 손실 함수의 합을 구하려면 시간이 걸린다. 빅데이터 수준이 되면 어마어마한 시간이 걸리는건 마찬가지.\n","\n","  => 빅데이터를 대상으로 일일이 손실함수 계산하는 것은 현실적이지 않음\n","<br/>\n","\n","- 그렇기에 훈련 데이터에서 일부만 골라 학습을 수행하며 데이터의 일부를 **미니배치**라고 한다\n","\n","  ex) 60000장 훈련 데이터에서 100장을 무작위로 뽑아 100장씩 학습 = 미니배치 학습\n","\n","  => 미니 배치 : 데이터 일부를 추려 전체의 '근사치'로 이용하는 것\n","\n"],"metadata":{"id":"Vt9G2UAhTlBX"}},{"cell_type":"code","source":["!git clone https://github.com/WegraLee/deep-learning-from-scratch.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TP2iS4pPsWJd","executionInfo":{"status":"ok","timestamp":1667375840687,"user_tz":-540,"elapsed":2003,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"876fe53c-fed7-4caf-d60e-38a214b1073b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'deep-learning-from-scratch'...\n","remote: Enumerating objects: 826, done.\u001b[K\n","remote: Total 826 (delta 0), reused 0 (delta 0), pack-reused 826\u001b[K\n","Receiving objects: 100% (826/826), 52.21 MiB | 39.54 MiB/s, done.\n","Resolving deltas: 100% (477/477), done.\n"]}]},{"cell_type":"code","source":["%cd /content/deep-learning-from-scratch/dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GoELB_hWVK05","executionInfo":{"status":"ok","timestamp":1667375841206,"user_tz":-540,"elapsed":23,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"19f9911d-8887-4ce3-a61d-08e4947b739f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/deep-learning-from-scratch/dataset\n"]}]},{"cell_type":"code","source":["import sys, os\n","sys.path.append(os.pardir)\n","import numpy as np \n","from dataset.mnist import load_mnist\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3sjMKfmUv1c","executionInfo":{"status":"ok","timestamp":1667375841600,"user_tz":-540,"elapsed":401,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"6b9e6b9c-ec44-43b3-cf47-6b7cd43d2ba2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Converting train-images-idx3-ubyte.gz to NumPy Array ...\n","Done\n","Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n","Done\n","Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n","Done\n","Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n","Done\n","Creating pickle file ...\n","Done!\n"]}]},{"cell_type":"code","source":["x_train.shape, t_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JWOxvBHJVCqu","executionInfo":{"status":"ok","timestamp":1667375841601,"user_tz":-540,"elapsed":34,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"b585d711-1ce0-495a-c84a-3305ad4cb8f8"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((60000, 784), (60000, 10))"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["- 이 훈련데이터에서 무작위로 10장만 뽑으려면 어떻게 해야할까??\n","\n","  => np.random.choice()\n","\n","  ㄴ 지정한 범위의 수 중에서 무작위로 원하는 개수만 꺼낼 수 있음\n","\n","  ㄴ 무작위로 선택한 인덱스를 사용 / 손실함수도 미니배치로 계산"],"metadata":{"id":"0EoER3r0WCht"}},{"cell_type":"code","source":["train_size = x_train.shape[0] # x_train.shape[0] = 60000\n","batch_size = 10\n","batch_mask = np.random.choice(train_size, batch_size) # np.random.choice(60000, 10) : 0 이상 60000미만의 수 중에서 무작위로 10개를 골라낸다.\n","x_batch = x_train[batch_mask] # 이 함수가 출력한 배열을 미니배치로 뽑아낼 데이터의 인덱스로 사용\n","t_batch = t_train[batch_mask]"],"metadata":{"id":"a12y7HE9VQ2V","executionInfo":{"status":"ok","timestamp":1667375841601,"user_tz":-540,"elapsed":28,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["- np.random.choice 사용시 뽑은 인덱스는 범위내에서 제거하는가??\n","\n","=> 복원 / 비복원 추출 기능 파라미터가 있음(replace = True/False)\n","\n","[[Python numpy] np.random.choice() 메소드로 임의표본 추출하기 (무작위, 확률 샘플링)](https://rfriend.tistory.com/548)"],"metadata":{"id":"NLvsU4YlSwVU"}},{"cell_type":"code","source":["np.random.choice(60000, 10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CntBw6eKWWT2","executionInfo":{"status":"ok","timestamp":1667375841601,"user_tz":-540,"elapsed":28,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"2521a23a-86a6-43ed-e383-b1a330bd7e2c"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([53582,  7143, 42699, 17939, 12545, 19536, 45142, 13450, 40350,\n","       56637])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["t_batch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeSryN7bWZYN","executionInfo":{"status":"ok","timestamp":1667375841602,"user_tz":-540,"elapsed":22,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"261d4221-50d7-431d-e189-1d6d877bb14e"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n","       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n","       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n","       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n","       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n","       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["- 미니배치의 손실 함수도 일부 표본 데이터로 전체와 비슷하게 계측한다. \n","\n","=> 전체 훈련 데이터의 대표로서 무작위로 선택한 작은 덩어리(미니배치)를 사용하는 것이다"],"metadata":{"id":"_gCPOuWdW47C"}},{"cell_type":"markdown","source":["### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기\n","\n","- 미니배치를 사용할 경우 교차 엔트로피 오차는 어떻게 구현할까??"],"metadata":{"id":"SEkshn0EXFHx"}},{"cell_type":"code","source":["def cross_entropy_error(y,t): \n","    if y.ndim == 1: \n","        t = t.reshape(1, t.size) \n","        y = y.reshape(1, y.size) # (10,) =>  (1, 10) / 1차원에서 2차원으로 변환 \n","       \n","        batch_size = y.shape[0] # y.shape[0] = 1\n","        return -np.sum(t * np.log(y + 1e-7)) / batch_size"],"metadata":{"id":"ir7_5p0cW1M-","executionInfo":{"status":"ok","timestamp":1667375841602,"user_tz":-540,"elapsed":17,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["- 왜 reshape해서 (10,)에서 (1, 10)으로, 차원을 늘려줄까?? \n","   \n","   => batch_size를 가진 데이터가 들어가면 ( batch_size, labels 개수 ) 이런 2차원 형태를 가진다\n","     \n","     ㄴ 2차원의 데이터가 들어오면 / if문이 작동하지 않고 넘어가서 / 함수내 batch_size에는 우리가 지정한 batch_size가 y.shape[0]에 그대로 들어간다\n","    \n","     ㄴ 하지만 1차원의 데이터의 경우 원래는 batch_size가 존재하지 않지만 /  batch_size가 1이기에 reshape를 통해서 2차원으로 늘려주고\n","    \n","     ㄴ 함수 내 batch_size에 1이 들어가도록 해줘서 배치사이즈가 있든 없든 모든 데이터를 처리할 수 있도록 만들었다"],"metadata":{"id":"ziFrQ_TDigIm"}},{"cell_type":"markdown","source":["> 위 코드는 데이터가 하나인 경우와 데이터가 배치로 묶여 입력될 경우 모두를 처리할 수 있도록 구현 되었다.\n","- y는 신경망의 출력 / t는 정답 레이블\n","- y가 1차원이라면 데이터 하나당 교차 엔트로피 오차를 구하는 경우는 reshape로 데이터 형태를 바꿔준다\n","- 배치의 크기로 나눠 정규화하고 이미지 1장당 평균 교차 엔트로피 오차를 계산한다"],"metadata":{"id":"DCmyqDvyXxCM"}},{"cell_type":"code","source":["a = np.arange(0,1,0.1)\n","b = np.arange(0,1,0.1)\n","ab = np.stack([a,b], axis=0)\n","ab.shape, ab.ndim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OcSW6QFBdyJ0","executionInfo":{"status":"ok","timestamp":1667375841602,"user_tz":-540,"elapsed":17,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"10377e86-d86e-4c77-9691-7ec780efcd02"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((2, 10), 2)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n","# y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n","t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])"],"metadata":{"id":"UhI0NYnMYWRi","executionInfo":{"status":"ok","timestamp":1667543086973,"user_tz":-540,"elapsed":2557,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["y.shape, y.reshape(1, y.size).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_DQTJ7n_dCQF","executionInfo":{"status":"ok","timestamp":1667375842022,"user_tz":-540,"elapsed":67,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"64705373-ac5a-4d29-d7b4-f969e6b688bb"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((10,), (1, 10))"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["cross_entropy_error(y,t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BPAXUsbSZNd8","executionInfo":{"status":"ok","timestamp":1667375842024,"user_tz":-540,"elapsed":65,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"598bca71-96a9-4126-f9b7-cb701bd31a86"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.510825457099338"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# 정답 레이블이 원-핫 인코딩이 아니라 숫자 레이블로 주어졌을 경우\n","def cross_entropy_error(y,t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","    \n","    batch_size = y.shape[0]\n","    # print(y[np.arange(batch_size), t])\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"],"metadata":{"id":"VPJM718MYWW0","executionInfo":{"status":"ok","timestamp":1667543577712,"user_tz":-540,"elapsed":5092,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["- -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size <= 이부분 어떤 형태로 나오는지 자세하게 알아야 할듯??"],"metadata":{"id":"zW0sQKNQsk3o"}},{"cell_type":"code","source":["batch_size =  5\n","np.arange(batch_size) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOEkhP5vrlhu","executionInfo":{"status":"ok","timestamp":1667375842032,"user_tz":-540,"elapsed":66,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"2473d9ad-5ae2-46f6-e137-dc7aff23787f"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2, 3, 4])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["t = np.array([2,7,0,9,4])\n","y = np.array([2,7,1,1,1])\n","[np.arange(batch_size), t]"],"metadata":{"id":"u9o9hBEUrurl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cross_entropy_error(y,t)"],"metadata":{"id":"W9o_OxwrUGI0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로, 그 계산은 무시해도 좋다는 것이 핵심\n","\n","- 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차 계산가능하다\n","\n","---\n","\n","- 원-핫 인코딩 시 t * np.log(y) 부분을 **숫자 레이블의 경우에는 np.log(y[np.arange(batch_size), t])로 구현**\n","\n","- np.arange(batch_size)는 0부터 batch_size -1까지 배열을 생성한다 \n","\n","  => batch_size가 5이면 np.arange(batch_size) = [0,1,2,3,4]\n","\n","- t = [2,7,0,9,4] 이면 y[np.arange(batch_size), t]는 각 데이터의 정답 레이블에 해당하는 신경망의 출력을 추출한다\n","\n"," => 여기 예제에서 y[np.arange(batch_size), t]는 [y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]]인 넘파이 배열 생성"],"metadata":{"id":"Z7bOymhkXKI1"}},{"cell_type":"markdown","source":["### 4.2.5 왜 손실 함수를 설정하는가??\n","\n","> 정확도란 지표를 두고 손실 함수를 쓰는 이유는???\n","\n","- '미분'의 역할에 주목하면 정확도 대신 손실함수 쓰는 이유를 알 수 있다\n","\n","- 신경망 학습에서는 최적의 매개변수(가중치,편향) 탐색 시 손실 함수 값 최소화하는 매개변수 값을 찾는다\n","\n","- 이때 매개변수의 미분(기울기)을 계산하고, 그 값을 단서로 매개변수의 값을 서서히 갱신하는 과정 반복한다\n","\n","- 미분 값의 양,음에 따라 반대방향으로 매개변수 변화시켜 손실함수 값 줄일 수 있다\n","\n","- 미분 값이 0이 되면 가중치 매개변수의 갱신이 멈추게 된다\n","\n","- 신경망을 학습할 때 정확도를 지표로 삼으면 안되는데, 정확도를 지표로 삼으면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다\n"],"metadata":{"id":"5i2x9iqGkV4k"}},{"cell_type":"markdown","source":["\n","\n","- 정확도는 매개변수의 미소한 변화에는 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불연속적으로 갑자기 변화한다\n","\n","- 그래서 계단함수를 활성화함수로 사용하지 않는 것과 같은 이유이다\n","\n","- 매개변수의 작은 변화가 주는 파장을 계단 함수가 말살하여 손실함수의 값에는 아무런 변화가 나타나지 않기 때문이다\n","\n","<br/>\n","\n","![이미지](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fspgrh%2Fbtq1WeRNLh3%2F2bQkh9s9Pq3k7QQ0XH0y8K%2Fimg.png)\n","ㄴ 계단함수는 대부분의 장소에서 기울기가 0이지만 시그모이드 함수의 기울기는 0이 아니다\n","\n","> 시그모이드 함수의 미분은 출력과 곡선의 기울기가 연속적으로 변한다 = 어느 장소라도 미분이 0이 되지 않는다 => 이는 신경망 학습에서 중요한 성질로 기울기가 0이 되지 않기에 신경망이 올바르게 학습할 수 있다\n"],"metadata":{"id":"i9zyJIVLka3v"}},{"cell_type":"markdown","source":["## 4.3 수치 미분"],"metadata":{"id":"JBBSnn0vmxs6"}},{"cell_type":"markdown","source":["### 4.3.1 미분\n","\n","![이미지](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FebWi25%2Fbtq10KicVzZ%2FuD4G2kLbpJEElk7gQ59zL0%2Fimg.jpg)"],"metadata":{"id":"kSjqB6L7m0yE"}},{"cell_type":"code","source":["# 나쁜 구현 예\n","\n","def numerical_diff(f,x):\n","    h = 1e-50\n","    return (f(x+h) - f(x)) / h"],"metadata":{"id":"zjLfORxEj9Ar","executionInfo":{"status":"ok","timestamp":1667375842036,"user_tz":-540,"elapsed":61,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["> 이 함수는 '함수 f와 함수 f에 넘길 인수 x' - 두 인수를 받는다. 그러나 개선해야 할 점이 2가지 있다\n","\n","<br/>\n","\n","1. h에는 가급적 작은 값 대입하고 싶어(h를 0에 무한히 가깝게) 1e-50 사용\n","\n","  =>  이값을 쓸 경우 : 반올림 오차문제가 발생한다(작은값 생략) \n","  \n","  ex) np.float32(1e-50) = 0.0\n","\n","  개선 포인트 : 미세한 값으로 10e-4사용, 이 값을 사용시 좋은 결과를 얻는다고 알려짐\n","\n","\n","<br/>\n","\n","2. f의 차분(임의 두 점에서의 함수 값들의 차이) 문제이다\n","\n","   x + h와 x 사이의 함수 f의 차분을 계산하지만, 이 계산에는 애당초 오차가 있음을 주의해야한다\n","\n","  '진정한 미분'은 x위치의 함수의 기울기(=접선)에 해당하지만 구현에서는 (x+h)와 x사이의 기울기이므로 엄밀히 일치하지는 않는다\n","\n","  이 차이는 h를 무한히 0으로 좁히는게 불가능하여 생기는 한계이다\n","\n","\n","수치 미분에는 오차가 포함되는데 이 오차를 줄이기 위해 (x+h)와 (x-h)일 때의 함수f의 차분을 계산하는 방법을 쓰기도 한다 = 중심차분 or 중앙차분"],"metadata":{"id":"IFFs0edrnZKz"}},{"cell_type":"code","source":["def numerical_diff(f,x): # x에 대해 미분\n","    h = 1e-4 # 0.001\n","    return (f(x+h) - f(x-h)) / (2*h) # 위의 구현과 차이점에 주목할것!!"],"metadata":{"id":"eyAcJLqnnNB7","executionInfo":{"status":"ok","timestamp":1667375842037,"user_tz":-540,"elapsed":62,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["### 4.3.2 수치 미분의 예\n","\n","2차함수 미분해보기 --> y = 0.01x^2 + 0.1x"],"metadata":{"id":"oLsPPTWspgqC"}},{"cell_type":"code","source":["def function_1(x):\n","    return 0.01*x**2 + 0.1*x"],"metadata":{"id":"5Oh7uKIknUfJ","executionInfo":{"status":"ok","timestamp":1667375842039,"user_tz":-540,"elapsed":64,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["import numpy as np \n","import matplotlib.pyplot as plt\n","\n","x = np.arange(0.0, 20.0, 0.1)\n","y = function_1(x)\n","plt.xlabel('x')\n","plt.ylabel('f(x)')\n","plt.plot(x,y)\n","plt.show()"],"metadata":{"id":"MDlEr1CPnUcB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# x=5, 10일 때의 함수의 미분\n","numerical_diff(function_1,5), numerical_diff(function_1,10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e85XntuMp6CK","executionInfo":{"status":"ok","timestamp":1667375842040,"user_tz":-540,"elapsed":61,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"f39b47b7-02ab-46bc-f799-ba174381e654"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.1999999999990898, 0.2999999999986347)"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["> 미분 값은 x에 대한 f(x)의 변화량 = 함수의 기울기\n","\n","x가 5와 10일때 완벽한 미분은 0.2와 0.3 => 오차가 매우 적어 거의 같은 값이라고 할 수 있다"],"metadata":{"id":"VRibl_r3qMCt"}},{"cell_type":"markdown","source":["### 4.3.3 편미분 \n","\n","- 추가 공부 필요할 듯\n","\n","앞의 예와 달리 변수가 2개라는 것에 주의\n","\n","f(x0, x1) = x0^2 + x1^2\n","\n","> 변수가 여럿인 함수에 대한 미분을 **편미분**이라 한다\n","\n","![이미지](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FcSNsH1%2Fbtq1ZFH83qa%2Fj4bsNVGWu1kijCXoKEskK1%2Fimg.jpg)"],"metadata":{"id":"Dxe-2m3ZqVHP"}},{"cell_type":"code","source":["def function_2(x):\n","    return x[0]**2 + x[1]**2\n","    # return np.sum(x**2)"],"metadata":{"id":"PE7zNHwsqIRs","executionInfo":{"status":"ok","timestamp":1667375842040,"user_tz":-540,"elapsed":56,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["편미분 풀어보기\n","\n","문제1 : x0=3, x1=4일때, x0에 대한 편미분 af/ax0를 구하라"],"metadata":{"id":"3XLruX5yq-uI"}},{"cell_type":"code","source":["def function_tmp1(x0):\n","    return x0*x0 + 4.0**2.0\n","\n","numerical_diff(function_tmp1, 3.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UKg8tO9KqpMm","executionInfo":{"status":"ok","timestamp":1667375842041,"user_tz":-540,"elapsed":56,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"bf0dac7e-321c-4521-fb11-053a5a875069"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.00000000000378"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["문제2 : x0 = 3, x1 = 4일때, x1에 대한 편미분 af/ax1을 구하라"],"metadata":{"id":"CW0JhcqFrZ2q"}},{"cell_type":"code","source":["def function_tmp2(x1):\n","    return 3.0**2.0 + x1*x1\n","\n","numerical_diff(function_tmp2, 4.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65ZVnvBMrMQt","executionInfo":{"status":"ok","timestamp":1667375842042,"user_tz":-540,"elapsed":48,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"2f2631d2-8567-4275-fdf5-bf9edef8db9c"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7.999999999999119"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["> 편미분을 푸는 방법\n","\n","- 변수가 하나인 함수를 정의하고 그 함수를 미분하는 형태로 구현하여 풀었다\n","\n","- 편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구한다\n","\n"," => 단, 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정"],"metadata":{"id":"0k4k1qqBrrbE"}},{"cell_type":"markdown","source":["## 4.4 기울기\n","\n","> x0과 x1의 편미분을 동시에 계산하려면 어떻게 해야할까??\n","\n","=> 기울기 : (af/ax0, af/ax1)처럼 모든 변수의 편미분을 벡터로 정리한 것"],"metadata":{"id":"wXq3unpCr83v"}},{"cell_type":"code","source":["def numerical_gradient(f,x):\n","    h = 1e-4\n","    grad = np.zeros_like(x) # x와 같은 0으로 된 배열 생성\n","\n","    for idx in range(x.size):\n","        tmp_val = x[idx]\n","        # f(x+h) 계산\n","        x[idx] = tmp_val + h\n","        fxh1 = f(x)\n","\n","        # f(x-h) 계산\n","        x[idx] = tmp_val - h\n","        fxh2 = f(x)\n","\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","        x[idx] = tmp_val # 값 복원\n","\n","    return grad"],"metadata":{"id":"bjiHk4Hmrj-f","executionInfo":{"status":"ok","timestamp":1667375842043,"user_tz":-540,"elapsed":43,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["numerical_gradient(f,x) 함수의 인수인 f는 함수이고 x는 넘파이 배열이므로 x의 각 원소에 대해서 수치 미분을 구한다"],"metadata":{"id":"dSeGjigXzBNo"}},{"cell_type":"code","source":["np.array([3.0, 4.0]).size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oynkgF7cuTnt","executionInfo":{"status":"ok","timestamp":1667375842046,"user_tz":-540,"elapsed":46,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"ee2c6935-3914-4314-aa7b-2fb038a80e92"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["numerical_gradient(function_2, np.array([3.0, 4.0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77pHnWYqy7vY","executionInfo":{"status":"ok","timestamp":1667375842047,"user_tz":-540,"elapsed":42,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"e8ffe782-148d-4c77-a80d-2eee42d614aa"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([6., 8.])"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["- 기울기 그림을 통해 의미 알아보기 (코드는 github 참고)\n","\n","![이미지](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FuLN5K%2Fbtq1ZECyRMI%2FiOWb1SwbRpgu3DigLepLyK%2Fimg.png)\n","\n","- 기울기의 결과에 마이너스를 붙인 벡터 그려보았다\n","\n","- 그림의 기울기는 함수의 가장 낮은 장소(최솟값)를 가리킨다\n","\n","- 최솟값에서 멀어질수록 화살표의 크기가 커진다\n","\n","- 그림처럼 반드시 기울기는 가장 낮은 장소를 가리키는 것은 아니다\n","\n","- **기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장크게 줄이는 방향**이다 <= 중요한 포인트이므로 꼭 기억할 것!!"],"metadata":{"id":"swl8YdW7zPqf"}},{"cell_type":"markdown","source":["### 4.4.1 경사법(경사하강법)\n","\n","> ❔ 최적의 매개변수(손실 함수가 최솟값이 되게 하는 매개변수 값들)를 학습 시에 찾아야 하는데 매개변수 공간은 아주 광범위해서 어디가 최솟값인지 찾기가 어렵다\n","\n","  \n","  💡 기울기를 이용해 함수의 최솟값을 찾으려는 것이 경사법\n","\n","<br/>\n","\n","❗ 주의할 점 : 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기라는 것이다\n","\n","그러나 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지 보장할 수는 없다\n","\n","실제로 복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없는 경우가 대부분\n","\n","ex) 함수가 극솟값, 최솟값, 안장점이 되는 장소에서는 기울기가 0\n","\n","ㄴ 안장점은 방향에 따라 극댓값이 될 수 있고 극솟값이 될 수 있음\n","\n","ㄴ 복잡한 함수는 평평한 곳으로 파고들면서 '고원'상태 (학습이 진행되지 않는상태)가 될 수 있음\n","\n","<br/>\n","\n","💡 현 위치에서 기울어진 방향으로 일정 거리만큼 이동하고 이동한 곳에서 기울기를 구하고, 또 기울어진 방향으로 나아가기를 반복하며 함수의 값을 점차 줄이는 것을 **경사법**이라고 한다\n","\n","\n","![이미지](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbP6FUT%2Fbtq127DDzmK%2FdqSvlXgScHa308CLIKHko0%2Fimg.jpg)\n","\n","- 경사법 수식 : n기호(에타)는 갱신하는 양 \n","\n"," = 학습률 : 한번의 학습으로 얼마나 학습할지, 매개변수 값을 얼마나 갱신할지 정하는 것\n","\n"," 학습률과 같은 매개변수를 하이퍼파라미터라고 한다 (사람이 직접 설정해야 하는 매개변수)"],"metadata":{"id":"xESsJwLez6Eg"}},{"cell_type":"code","source":["def gradient_descent(f, init_x, lr=0.01, step_num=100):\n","    x = init_x\n","\n","    for i in range(step_num):\n","        grad = numerical_gradient(f,x)\n","        x -= lr * grad\n","\n","    return x"],"metadata":{"id":"X2bBUy0lz59b","executionInfo":{"status":"ok","timestamp":1667375842672,"user_tz":-540,"elapsed":663,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["\n","- f는 최적화하려는 함수 / init_x는 초깃값  \n","- lr은 learning rate(학습률) / step_num은 경사법에 따른 반복 횟수 => 로지스틱회귀 등의 iter_max?\n","- 함수의 기울기는 numerical_gradient(f,x)로 구하고 / 그 기울기에 학습률을 곱한 값으로 갱신하는 처리를 step_num번 반복한다\n"," "],"metadata":{"id":"DggvFlTy2pPA"}},{"cell_type":"markdown","source":["경사법으로 f(x0, x1)= x0^2+x1^2의 최솟값 구하기"],"metadata":{"id":"U69WZEjh3CXB"}},{"cell_type":"code","source":["def function_2(x):\n","    return x[0]**2 + x[1]**2\n","\n","init_x = np.array([-3.0, 4.0]) \n","gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-SAwKKq2Plt","executionInfo":{"status":"ok","timestamp":1667375842674,"user_tz":-540,"elapsed":67,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"f216f8fb-8f4b-421c-9d33-52695dcb487c"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-6.11110793e-10,  8.14814391e-10])"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["- 초깃값을 (-3.0, 4.0) 으로 설정한 후 경사법을 사용해 최솟값 탐색을 시작\n","- [-6.11110793e-10,  8.14814391e-10], 거의 (0,0)에 가까운 결과로 경사법으로 거의 정확한 결과를 얻은 것"],"metadata":{"id":"0UIjOmvQ3Xeg"}},{"cell_type":"markdown","source":["![이미지](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FlCGSD%2Fbtq11LVd9lU%2FmdkieKHef3KxriTCJCkU90%2Fimg.png)"],"metadata":{"id":"D-YUa30D3o5s"}},{"cell_type":"code","source":["%cd /content/deep-learning-from-scratch/ch04"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_C0itTOr33Ss","executionInfo":{"status":"ok","timestamp":1667375842677,"user_tz":-540,"elapsed":65,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"d9979d1c-a54a-4a61-c98e-4d0c0e7391d9"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/deep-learning-from-scratch/ch04\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pylab as plt\n","from gradient_2d import numerical_gradient\n","\n","\n","def gradient_descent(f, init_x, lr=0.01, step_num=100):\n","    x = init_x\n","    x_history = []\n","\n","    for i in range(step_num):\n","        x_history.append( x.copy() )\n","\n","        grad = numerical_gradient(f, x)\n","        x -= lr * grad\n","\n","    return x, np.array(x_history)\n","\n","\n","def function_2(x):\n","    return x[0]**2 + x[1]**2\n","\n","init_x = np.array([-3.0, 4.0])    \n","\n","lr = 0.1\n","step_num = 20\n","x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n","\n","plt.plot( [-5, 5], [0,0], '--b')\n","plt.plot( [0,0], [-5, 5], '--b')\n","plt.plot(x_history[:,0], x_history[:,1], 'o')\n","\n","plt.xlim(-3.5, 3.5)\n","plt.ylim(-4.5, 4.5)\n","plt.xlabel(\"X0\")\n","plt.ylabel(\"X1\")\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"rT-l_bUczJaC","executionInfo":{"status":"ok","timestamp":1667375842680,"user_tz":-540,"elapsed":63,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"c13cb4d9-bb88-474f-8cc3-300cbd644d23"},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["> 학습률 적절히 설정하는 것이 중요\n","\n","1. 학습률이 너무 클경우 최솟값을 찾지 못하고 큰 값으로 발산한다\n","2. 학습률이 너무 작을경우 거의 갱신되지 않은 채 반복종료될 수 있다\n","\n","=> 학습률과 같은 매개변수를 하이퍼 파라미터라고 한다\n","\n","   사람이 직접 설정하는 매개변수\n","\n","   여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 값을 찾는 과정 거쳐야 한다"],"metadata":{"id":"UFNfaqbgNp5g"}},{"cell_type":"code","source":["init_x = np.array([-3.0, 4.0])\n","gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"],"metadata":{"id":"PULg_TIh3xH4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"],"metadata":{"id":"8LcGSa-pNUMI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.4.2 신경망에서의 기울기\n","\n","ex) 형상이 2X3, 가중치가 W, 손실함수가 L인 신경망 (경사는 aL/aW)\n","\n","![이미지](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FCCjLZ%2Fbtq11zm5BHJ%2FKtmD834pRokDdmn7GX6E1K%2Fimg.jpg)\n","\n","aL/aW의 각 원소는 각각의 원소에 대한 편미분\n","\n","여기서 중요한 점은 aL/aW의 형상은 W와 같다는 것이다."],"metadata":{"id":"zgrxQf0jOKQ7"}},{"cell_type":"code","source":["# coding: utf-8\n","import sys, os\n","sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n","import numpy as np\n","from common.functions import softmax, cross_entropy_error\n","from common.gradient import numerical_gradient\n","\n","\n","class simpleNet:\n","    def __init__(self):\n","        self.W = np.random.randn(2,3) # 정규분포로 초기화\n","\n","    def predict(self, x):\n","        return np.dot(x, self.W)\n","\n","    def loss(self, x, t):\n","        z = self.predict(x)\n","        y = softmax(z)\n","        loss = cross_entropy_error(y, t)\n","\n","        return loss\n","\n","# x = np.array([0.6, 0.9])\n","# t = np.array([0, 0, 1])\n","\n","# net = simpleNet()\n","\n","# f = lambda w: net.loss(x, t)\n","# dW = numerical_gradient(f, net.W)\n","\n","# print(dW)"],"metadata":{"id":"SGcWYPyxNaab","executionInfo":{"status":"ok","timestamp":1667375842683,"user_tz":-540,"elapsed":47,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["> simpleNet 클래스\n","\n","- 형상이 2X3인 가중치 매개변수 하나를 인스턴스 변수로 가짐\n","- 메서드 : 예측수행(predict), 손실함수계산(loss)\n","- 인수 x는 입력데이터, t는 정답레이블"],"metadata":{"id":"srXW-m5oPBkx"}},{"cell_type":"code","source":["net = simpleNet()\n","net.W"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LwboaYL8Ovtz","executionInfo":{"status":"ok","timestamp":1667375842684,"user_tz":-540,"elapsed":47,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"a5979ef8-c6dc-4e73-fbb3-3be92d650748"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 3.0727667 , -0.44964474, -0.12648355],\n","       [ 0.41320474,  1.65401221, -1.08429091]])"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["x = np.array([0.6, 0.9])\n","p = net.predict(x)\n","p"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0LaJRcpFPQMz","executionInfo":{"status":"ok","timestamp":1667375842686,"user_tz":-540,"elapsed":40,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"9fa42f02-ca97-4dbd-8d39-4ece6d7fd578"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 2.21554428,  1.21882414, -1.05175194])"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["np.argmax(p)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sy38XhmmPhWH","executionInfo":{"status":"ok","timestamp":1667375842687,"user_tz":-540,"elapsed":37,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"d160f133-1287-462e-d465-03c2265a71dd"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["t = np.array([0,0,1])\n","net.loss(x,t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PAiH75JcPk_E","executionInfo":{"status":"ok","timestamp":1667375842689,"user_tz":-540,"elapsed":34,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"488048eb-4751-49f7-e57f-155d734d944c"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.6088925573864237"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["- 기울기 구하기"],"metadata":{"id":"yh3wsHD5PstO"}},{"cell_type":"code","source":["def f(W): # net.W를 인수로 받아 손실 함수를 계산하는 새로운 함수 f\n","    return net.loss(x,t)\n","\n","dW = numerical_gradient(f, net.W) # numerical_gradient(f,x)의 인수f는 함수 / x는 함수 f의 인수 \n","dW"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fo4Z9pwXPpwr","executionInfo":{"status":"ok","timestamp":1667375843166,"user_tz":-540,"elapsed":504,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"d5285217-e5e4-4748-f550-a73e9f98db5d"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.42637786,  0.15737096, -0.58374881],\n","       [ 0.63956678,  0.23605643, -0.87562322]])"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["dW는 numerical_gradient(f, net.W)의 결과로 2x3 2차원배열.\n","\n","첫번째 기울기는 w11을 h만큼 늘리면 손실 함수의 값은 0.25h만큼 증가한다는 의미\n","\n","마지막 기울기는 w23을 h만큼 늘리면 손실 함수의 값은 0.6만큼 감소\n","\n","=> w23은 양의 방향으로 / w11은 음의 방향으로 갱신해야 한다.\n","\n","=> 기울기 값이 클수록 갱신되는 양에 크게 기여한다."],"metadata":{"id":"o_vlpXJBwHGQ"}},{"cell_type":"code","source":["# lambda 기법 사용하면 더 편리하게 구현가능 (간단한 함수의 경우만 사용)\n","f = lambda w: net.loss(x, t)\n","dW = numerical_gradient(f, net.W)"],"metadata":{"id":"CCDKNvqSP2_O","executionInfo":{"status":"ok","timestamp":1667375843166,"user_tz":-540,"elapsed":25,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["## 4.5 학습 알고리즘 구현하기\n","\n","- 주요 키워드 : 손실 함수, 미니배치, 기울기, 경사 하강법\n","\n","---\n","\n","### 신경망 학습 순서\n","> 신경망에는 적응 가능한 가중치, 편향이 있고 / 이 매개변수를 훈련데이터에 적응하도록 조정하는 과정을 **학습**이라 한다. \n","\n","> **신경망의 학습은 4단계로 수행된다**\n","\n","1. 미니배치\n","  \n","  - 훈련 데이터 중 일부를 무작위로 가져온, 선별한 데이터를 미니배치라 하며 / 미니배치의 손실 함수 값을 줄이는 것이 목표이다\n","\n","2. 기울기 산출\n","\n","  - 미니배치의 손실 함수 값을 줄이기위해 각 가중치 매개변수의 기울기를 구한다\n","\n","  - 기울기는 손실 함수 값을 가장 작게 하는 방향으로 제시한다\n","\n","3. 매개변수 갱신\n","\n","  - 가중치 매개변수를 기울기 방향으로 조금 갱신한다\n","\n","4. 반복\n","\n","  - 최솟값에 이르기까지 1~3단계를 반복한다\n","\n","\n","=> 경사 하강법으로 매개변수를 갱신한다\n","\n","=> 데이터를 미니배치로 무작위로 선정하기에 확률적 경사 하강법(SGD)라고 한다"],"metadata":{"id":"h73R4dDSxiRE"}},{"cell_type":"markdown","source":["### 4.5.1 2층 신경망 클래스 구현하기"],"metadata":{"id":"Mop_Ky16zPlY"}},{"cell_type":"code","source":["import sys, os\n","sys.path.append(os.pardir)\n","from common.functions import *\n","from common.gradient import numerical_gradient\n","\n","# input_size : 입력층 뉴런 수 |  hidden_size : 층 사이의 노드 개수 = 은닉층의 뉴런수 |  output_size : 마지막 출력 개수 = 출력층의 뉴런 수\n","\n","class TwoLayerNet:\n","    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): # 초기화 수행.\n","        # 가중치 초기화 \n","        self.params = {} # params : 신경망의 매개변수(가중치, 편향) 보관하는 딕셔너리 변수\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) \n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size) # 0으로 하는거 보면 편향은 계산에 안넣었나??\n","\n","    def predict(self, x): # 예측(추론) 수행. 인수 x는 이미지 데이터\n","        W1, W2 = self.params['W1'], self.params['W2']\n","        b1, b2 = self.params['b1'], self.params['b2']\n","\n","        a1 = np.dot(x,W1) + b1\n","        z1 = sigmoid(a1)\n","        a2 = np.dot(z1,W2) + b2\n","        y = softmax(a2)\n","\n","        return y\n","\n","    # x : 입력할 이미지 데이터 / t : 정답 레이블\n","    def loss(self, x, t): # 손실함수 값 계산.\n","        y = self.predict(x)\n","        return cross_entropy_error(y, t)\n","\n","    def accuracy(self, x, t): # 정확도 계산\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        t = np.argmax(t, axis=1)\n","\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy  \n","\n","    def numerical_gradient(self, x, t): # 가중치 매개변수 기울기 계산\n","        loss_W = lambda W: self.loss(x,t) \n","\n","        grads = {} # grads : 기울기 보관하는 딕셔너리 변수 (각 가중치와 편향에 따른 기울기를 계산하여 보관)\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])        \n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])        \n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])        \n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])        \n","\n","        return grads"],"metadata":{"id":"KsRD71tixDX_","executionInfo":{"status":"ok","timestamp":1667375843167,"user_tz":-540,"elapsed":25,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["- np.random.randn() : 표준정규분포 (Standard normal distribution)로부터 샘플링된 난수를 반환 \n","\n","- [NumPy 난수 생성 (Random 모듈)\n","](https://codetorial.net/numpy/random.html)"],"metadata":{"id":"RThvf6lh3wYv"}},{"cell_type":"code","source":["np.random.randn(2,3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e7OJSbB5zfGS","executionInfo":{"status":"ok","timestamp":1667375843167,"user_tz":-540,"elapsed":25,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"b33de9b4-310c-483a-a98b-f4ba120f953b"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-3.49354259,  0.32716369, -0.72547503],\n","       [-0.03258055, -0.57953955, -0.79906685]])"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n","net.params['W1'].shape, net.params['b1'].shape, net.params['W2'].shape, net.params['b2'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfVDMT3WzheK","executionInfo":{"status":"ok","timestamp":1667375843168,"user_tz":-540,"elapsed":20,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"4d81271a-7719-4902-9882-22892a55ef8a"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((784, 100), (100,), (100, 10), (10,))"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["- 이렇게 params 변수에 신경망에 필요한 매개변수를 모두 저장하고 예측 처리(순방향 처리)에 사용된다\n","\n","- 예측처리는 아래와 같이 실행한다"],"metadata":{"id":"9xiRh8FM7sFM"}},{"cell_type":"code","source":["x = np.random.rand(100, 784) # 예측용 임의의 데이터 생성\n","y = net.predict(x)\n"],"metadata":{"id":"MuGy-nHW7ajU","executionInfo":{"status":"ok","timestamp":1667375843169,"user_tz":-540,"elapsed":17,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["x = np.random.rand(100, 784) # 더미 입력 데이터 100장\n","t = np.random.rand(100, 10) # 더미 정답 레이블 100장\n","\n","grads = net.numerical_gradient(x,t) # 기울기 계산 - 2분 47초 걸림\n","\n","grads['W1'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VyZ_SqFV7-91","executionInfo":{"status":"ok","timestamp":1667376075612,"user_tz":-540,"elapsed":232459,"user":{"displayName":"두루미","userId":"06775262425078058900"}},"outputId":"43fa0b3d-d7e3-4790-e2d5-3aa5817d7cdb"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(784, 100)"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["- numerical_gradient(self, x,t)는 수치미분 방식으로 매개변수 기울기 계산\n","\n","- 하지만 다음장에서 사용할 오차역전파법을 사용하면 기울기 계산을 고속으로 수행할 수 있다"],"metadata":{"id":"m2evd3We84FJ"}},{"cell_type":"markdown","source":["### 4.5.2 미니배치 학습 구현하기"],"metadata":{"id":"V5tQJboh9JMc"}},{"cell_type":"code","source":["import numpy as np \n","from dataset.mnist import load_mnist \n","from two_layer_net import TwoLayerNet\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","train_loss_list = []\n","\n","iters_num = 1000 \n","train_size = x_train.shape[0] \n","batch_size = 100\n","learning_rate = 0.1\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","for i in range(iters_num): # 파이토치 형태와 비슷하다\n","    # 미니배치\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","\n","    # 기울기 계산\n","    grad = network.numerical_gradient(x_batch, t_batch)\n","\n","    # 매개변수 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2'):\n","        network.params[key] -= learning_rate*grad[key]\n","\n","    # 학습 경과 기록\n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)"],"metadata":{"id":"Mbd0urKs8Y9b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.5.3 시험 데이터로 평가하기\n","\n","- 학습할수록 손실 함수의 값(훈련 데이터의 미니배치에 대한 손실 함수)이 내려가는걸 볼 수 있지만 이것만으로 다른 데이터에서 비슷한 결과가 나온다고는 할 수 없다\n","\n","- 오버피팅을 일으키지 않는지를 확인해야 한다\n","\n","- 신경망 학습의 목표는 범용적인 능력을 익히는 것"],"metadata":{"id":"jg7CgXwj-jhs"}},{"cell_type":"code","source":["import numpy as np \n","from dataset.mnist import load_mnist \n","from two_layer_net import TwoLayerNet\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","iters_num = 1000 \n","train_size = x_train.shape[0] \n","batch_size = 100\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","# 1에폭당 반복 수 \n","iter_per_epoch = max(train_size / batch_size, 1) # 어떻게 나올까??\n","\n","for i in range(iters_num): # 파이토치 형태와 비슷하다\n","    # 미니배치\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","\n","    # 기울기 계산\n","    grad = network.numerical_gradient(x_batch, t_batch)\n","\n","    # 매개변수 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2'):\n","        network.params[key] -= learning_rate*grad[key]\n","\n","    # 학습 경과 기록\n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)\n","\n","    # 1에폭당 정확도 계산\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuarcy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print('train acc, test acc | ' + str(train_acc) + ', ' + str(test_acc))"],"metadata":{"id":"nrI_sA0r-WUO","executionInfo":{"status":"aborted","timestamp":1667376529708,"user_tz":-540,"elapsed":17,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 여기서는 1에폭당 모든 훈련 데이터와 시험 데이터에 대한 정확도를 계산하고 그 결과를 기록하기에 시간이 오래 걸린다. 에폭당 정확도의 추이를 확인하기 위한 목적으로 구현하였다"],"metadata":{"id":"Jf4uujM0AQyA"}},{"cell_type":"markdown","source":["## 4.6 정리\n","\n","- 기계학습에서는 훈련 데이터와 시험 데이터로 나눠 사용\n","- 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가\n","- 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다\n","- 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다 => 음이든 양이든\n","- 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라 한다\n","- 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다\n","- 수치 미분 계산은 오래걸리지만 구현이 간단하다 <=> 오차역전파법은 기울기를 고속으로 구할 수 있다"],"metadata":{"id":"KWeq_9-NAfIN"}},{"cell_type":"code","source":[],"metadata":{"id":"CCWQs1ZsAOG6","executionInfo":{"status":"aborted","timestamp":1667376529709,"user_tz":-540,"elapsed":17,"user":{"displayName":"두루미","userId":"06775262425078058900"}}},"execution_count":null,"outputs":[]}]}